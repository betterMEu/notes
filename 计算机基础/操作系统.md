

1. 一个软件程序

2. 管理计算机硬件与软件的程序

   > 应用程序都通过操作系统来调用系统内存以及磁盘等等硬件

3. 屏蔽了硬件层的复杂性

   > 操作系统就像是硬件使用的负责人，统筹着各种相关事项



内核（Kernel）是操作系统的核心，是连接应用程序和硬件的桥梁。

![Kernel_Layout](assets/Kernel_Layout.png)



## 基本特征

### 1.并发\并行

- 并发是指在一段时间内能同时运行的多个程序
- 并行则指同一时刻能运行多个指令



### 2.共享

指系统中的资源被线程间使用的方式。

- **互斥共享**

  资源称为==临界资源==，同一时刻只允许一个进程访问

- **同时共享**



### 3.虚拟

虚拟把一个物理实体转换为多个逻辑实体。

两种虚拟技术：

- 时分复用技术
- 空分复用技术

> ==并发执行使用了时分复用技术==，让每个进程轮流占用处理器，每次只执行一小个时间片并快速切换。

> ==虚拟内存使用了空分复用技术==，他将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页切换到内存中。



### 4.异步

异步指进程不是一次性执行完毕，而是走走停停，已不可知的速度向前推进。





## 基本功能

### 1.进程管理

进程控制、进程同步、进程通信、死锁处理、处理机调度等



### 2.内存管理

内存分配、地址映射、内存保护与共享、虚拟内存等。



### 3.文件管理

文件存储空间的管理、目录管理、文件读写管理和保护等。



### 4.设备管理

完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率。





## 系统调用

进程在系统上的运行分为两个级别：用户态和系统态。

如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而==陷入==内核，由操作系统代为完成。

> 陷入，属于中断的一种。



## 宏内核和微内核

#### 1. 宏内核

将操作系统功能作为一个紧密结合的整体放到内核。

由于各模块共享信息，因此有很高的性能。



#### 2. 微内核

由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。

在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。

因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。





## 中断分类

### 1. 外中断

由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。

此外还有时钟中断、控制台中断等。



### 2. 异常

由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。



### 3. 陷入

在用户程序中使用系统调用。





## 进程

### 进程和线程

#### 1.进程

进程是资源分配的基本单位。

进程控制块（Process Control Block,PCB）描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对PCB的操作。



#### 2. 线程

线程是独立调度的基本单位。

一个进程中可以有多个线程，它们共享进程资源。



#### 3.区别

Ⅰ 拥有资源

进程是资源分配的基本单位，所以线程不拥有资源，线程可以访问隶属于进程的资源。



Ⅱ 调用

线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程的线程切换到另一个进程的线程时，会引起进程切换。



Ⅲ 系统开销

创建或者撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O设备等，付出的开销远大于线程创建或撤销的开销。类似地，进程切换时，涉及当前进程CPU环境的保存及新调度进程的CPU环境的设置，而线程切换时只需要保存和设置很少量寄存器内容，开销很小。



Ⅳ 通信方面

线程间可以通过直接读写同一进程中的数据进行通信，但是进程间通信需要借助 IPC。





### 进程状态

- **创建状态(created)** ：创建
- **就绪状态(ready)** ：准备运行状态，获得了除了处理器之外的一切所需资源，得到即可运行
- **运行状态(running)** ：在处理器上上运行
- **阻塞状态(waiting)** ：等待状态，如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行
- **结束状态(terminated)** ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行

![process-state](assets/d38202593012b457debbcd74994c6292.png)

应该注意以下内容：

1. 只有就绪态和运行态可以相互转换，其它的都是单向转换。
2. 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。





### 进程调度算法

不同环境的调度算法目标不同，因此需要针对不同环境来探讨调度算法。



#### 批处理系统

批处理系统没有太多的用户操作，该系统中，调度算法的目标时保证吞吐量和周转时间（从提交到终止的时间）



##### 先来先服务 first-come first-serverd（FCFS）

按照请求顺序进行调度。长作业先到，短作业等待会久。



##### 最短工作优先 shortest job first（SJF）

按估计运行时间最短的顺序进行调度。不利于长作业



#####  最短剩余时间优先 shortest remaining time next（SRTN）

按剩余运行时间的顺序进行调度。 

> 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。



#### 分时系统

分时系统有大量的用户交互操作，在该系统中调度算法的目标是==快速地进行响应。==



##### 时间片轮转

时分复用技术。

就绪进程按照先来先服务（FCFS）的原则排进一个队列（Queue），每次调度时，把 CPU 时间分配给队首进程，执行一个时间片。

当时间片用完时，由计时器发出==时钟中断==，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。

> 算法效率和时间片的大小有关系：
>
> - 时间片太小，进程切换得太频繁，在进程切换上就会花过多时间。
> - 而如果时间片过长，那么实时性就不能得到保证。



##### 优先级调度

为每个进程分配一个优先级，按照优先级进行调度。

> 为了防止低优先级的进程永远得不到调度，可以随着时间的推移增加等待进程的优先级。
>



##### 多级反馈队列

一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。

多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。

<font color='red'>每个队列优先权也不同</font>，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。

可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。



#### 实时系统

实时系统要求一个请求在一个确定时间内得到响应。

分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。





### 进程通信

进程同步与进程通信很容易混淆，它们的区别在于：

- 进程同步：控制多个进程按一定顺序执行；
- 进程通信：进程间传输信息。

进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传输一些进程同步所需要的信息。



#### 1. 管道(Pipes)

限制：

- 只支持半双工通信（单向交替传输）
- 只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。



管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。

```c
#include <unistd.h>
int pipe(int fd[2]);
```



#### 2. 命名管道(Names Pipes)

去除了管道只能在父子进程中使用的限制



#### 3. 消息队列



#### 4. 信号量



#### 5. 共享存储

允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。

需要使用信号量用来同步对共享存储的访问。

多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用内存的匿名段。



#### 6. 套接字（Socket）

IP + PORT





### 进程同步

#### 1. 临界区

对临界资源进行访问的那段代码称为临界区。

每个进程在进入临界区之前，需要进行判断是否有进程在临界区。

```
entry section
critical section;
exit section
```



#### 2. 信号量

一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。

- **down** : 如果信号量大于 0 ，执行 -1 操作；等于 0，进程睡眠；
- **up** ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。

down 和 up 操作需要被设计成原子，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。

> 信号量的取值只能为 0 或者 1，那么就成为了 ==**互斥量（Mutex）**== 
>



#### 3. 管程

管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易。

> 在一个时刻只能有一个进程使用管程。
>
> 进程在无法继续执行的时候不能一直占用管程，否则其它进程永远不能使用管程。

管程引入了**wait()** 和 **signal()** 来实现同步操作

- wait() 会导致调用进程阻塞，把管程让出来给另一个进程持有
- signal() 操作用于唤醒被阻塞的进程









## 死锁

必要条件

- **互斥**：资源是临界资源
- **不可抢占**：线程已有资源不可被抢占
- **占有并等待**：线程已有资源不释放，同时请求新的资源而等待
- **循环等待**：有两个或者两个以上的线程都在等待下一个进程所占有的资源。

多个进程竞争有限数量的资源，当一个进程申请资源时，如果这时没有可用资源，那么这个进程进入等待状态。如果所申请的资源被其它等待进程占有，那么该等待进程有可能再也无法改变状态。

![线程死锁示意图 ](assets/2019-4死锁1.png)



处理方法

- 鸵鸟策略
- 死锁检测与死锁恢复
- 死锁预防
- 死锁避免



### 鸵鸟策略

把头埋在沙子里，假装根本没发生问题。

> 解决死锁的代价大，忽略他。
>



### 死锁预防

在==程序运行之前预防==发生死锁，只要让死锁四个必要条件其中一种不满足即可

1. **破坏互斥**

   

2. **破坏占有和等待**

   线程请求资源时一次性申请所有资源

   

3. **破坏不可抢占**

   线程进一步申请其他资源时，申请失败，可以主动释放它占有的资源

   

4. **破坏循环等待**

   给资源统一编号，进程只能按编号顺序来请求资源，若不可获取则等待。释放资源则反序释放。


> 注：通常考虑破坏2，4条件
>



### 死锁避免

程序代码中避免死锁

##### 1. 安全状态

系统的状态分为 **安全状态** 和 **不安全状态** ，每当在分配资源前先测试系统状态

Has 表示已拥有的资源数

Max 表示总共需要的资源数

Free 表示还有可以使用的资源数

从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。

定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。

安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。



##### 2. 单个资源的银行家算法

一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。




上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。

##### 3. 多个资源的银行家算法



上图中有五个进程，四个资源。

左边的图表示已经分配的资源，右边的图表示还需要分配的资源。

最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。

检查一个状态是否安全的算法如下：

- 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。
- 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。
- 重复以上两步，直到所有进程都标记为终止，则状态时安全的。

如果一个状态不是安全的，需要拒绝进入这个状态。



### 死锁检测与恢复

运行时检测到死锁发生时，采取措施进行恢复。

检测步骤：

1. 如果进程-资源分配图中**无环路**，则此时系统没有发生死锁
2. 如果进程-资源分配图中**有环路**，且每个资源类仅有一个资源，则系统中已经发生了死锁。
3. 如果进程-资源分配图中**有环路**，且涉及到的资源类有多个资源，此时系统未必会发生死锁。如果能在进程-资源分配图中找出一个 **既不阻塞又非独立的进程** ，该进程能够在有限的时间内归还占有的资源，也就是把边给消除掉了，重复此过程，直到能在有限的时间内 **消除所有的边** ，则不会发生死锁，否则会发生死锁。(消除边的过程类似于 **拓扑排序**)



##### 1. 每种类型一个资源的死锁检测





上图为资源分配图，资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。

图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。

每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。



##### 2. 每种类型多个资源的死锁检测



上图中，有三个进程四个资源，每个数据代表的含义如下：

- E 向量：资源总量
- A 向量：资源剩余量
- C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量
- R 矩阵：每个进程请求的资源数量

进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。

算法总结如下：

每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。

1. 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。
2. 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。
3. 如果没有这样一个进程，算法终止。



##### 3. 死锁恢复

1. ###### 抢占恢复

   抢占资源，从涉及死锁的一个或几个进程中抢占资源，把夺得的资源再分配给涉及死锁的进程直至死锁解除

   

2. ###### 回滚恢复

   逐个撤销涉及死锁的进程，回收其资源直至死锁解除

   

3. ###### 结束进程恢复

   撤销涉及死锁的所有进程，解除死锁后继续运行 

   

4. ###### 重启






## 内存管理

- 负责内存的分配和回收
- 地址转换



### 内存管理方式

#### 块式管理

空间大，每个块中未被利用的空间，称之碎片。



#### 页式管理

页较小，提高了内存利用率，减少了碎片。

相比分段式内存管理，可能大家对分页式内存管理要熟悉的多。

操作系统将内存空间按照“页”为单位划分了很多页面，这个页的大小默认是4KB（当然可以改的），各进程拥有虚拟的完整的地址空间，进程中使用到的页面会映射到真实的物理内存上，程序中使用的地址是**虚拟地址**，CPU在运行时自动将其翻译成真实的物理地址。

![img](assets/v2-9e63c747b41a5bd8ff3e735dd2a3b555_720w.jpg)

既然要翻译，那就得有地方记录虚拟地址和物理地址的映射关系，只有根据这个关系，才能完成翻译。

这个映射关系，是通过页表来完成的。

页表是用来记录虚拟内存页面和物理内存页面之间的映射关系的，每一个页表项记录一个页面的映射关系。但进程的地址空间很大，这样算下来需要的页表项的数量也会非常多。而实际上进程地址空间中很多页面都没有真正使用，也就没有映射关系，这样是一种浪费。

为了解决这个问题，CPU引入了多级页表的机制，在32位下一般是2级页表，像下面这样：

![img](assets/v2-c666284e8afa9d19bfcd9b5bb19a7f8b_720w.jpg)

将虚拟地址划分了三段：页目录索引、页表索引、页内偏移。

线程切换时，如果同时发生了进程切换，CPU中的CR3寄存器将会加载当前进程的页目录地址。

在寻址的时候，通过CR3，一级一级按表索页，最终找到对应的物理内存页面，再结合页面内的偏移值，实现最终的内存寻址。



#### 分段式内存管理

早在16位的8086时代，CPU为了能寻址超过16位地址能表示的最大空间（64KB），引入了==段寄存器==。

通过将内存空间划分为若干个段，然后采用==段基地址+段内偏移==的方式访问内存，这样能访问1MB的内存空间了！

![image-20220823200151866](assets/image-20220823200151866.png)

那时候，段寄存器有4个，分别指向不同的段。

> **cs**: 代码段
> **ds**: 数据段
> **ss**: 栈段
> **es**：扩展段

在那个时候，段寄存器存放的是一个段基地址。

在通过ip寄存器读取指令的时候，实际上是cs:ip，通过sp寄存器访问栈的时候，实际上是ss:sp。



**变化1：**

在32位时代，段寄存器又增加了两个：fs、gs，这两个段寄存器有特殊用途。



**变化2：**

段寄存器里面存放的不再是段基地址，而是一个叫**段选择子**的东西。注意，注意，一切的变化都从这里开始。

段寄存器是16位的宽度，原来这16位是个物理内存地址，但现在，它是这样一个结构：

![image-20220823200529156](assets/image-20220823200529156.png)

实际上，现在的段寄存器中存放的是一个号码，什么号码呢？是一个表格中表项的号码，这个表，有可能是**全局描述符表GDT**，也有可能是**局部描述符表LDT**。

那到底是哪个表？是由段选择子从低到高的第三位来决定的，如果这一位是0，则是GDT，否则就是LDT。

那这两个表又是啥，表里面装的又是什么，怎么来寻址呢？

这两个表的表项叫做段描述符，描述了一个内存段的信息，比如段的基地址、最大长度、访问属性等等一系列信息，它长这个样子：

![img](assets/v2-a7b5343c75aa24092931b0182f5941f1_720w.jpg)

CPU中单独添置了两个寄存器，用来指向这两个表，分别是gdtr和ldtr。

在寻址的时候，CPU首先根据段寄存器中的号码，通过gdtr或ldtr来到GDT/LDT中取出对应的段描述符，然后再取出这个段的基地址，最后再结合段内的偏移，完成内存寻址。



![img](assets/v2-ab7b854caedd18bee0363438019c9c47_720w.jpg)

也就是说，在16位模式下，段寄存器中直接就是一个地址，相当于一个指针，而到了32位下，则变成了一个句柄，或者说二级指针了。







#### 段页式管理

段页式管理机制就是把主存先分成若干段，每个段又分成若干页

分页满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要







### 页表管理

#### 多级页表

引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间（时换空）

#### 快表

页表方案 基础之上引入了 **快表** 来加速虚拟地址到物理地址的转换

由于多级页表造成了时间性能牺牲，引入快表





### 内存地址

C 语言中，指针里面存储的数值就是逻辑地址，逻辑地址由操作系统决定。物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址

CPU 寻址：虚拟寻址，CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。  CPU 中含有一个被称为 **内存管理单元（Memory Management Unit, MMU）** 的硬件

虚拟地址空间：不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存



### 虚拟内存

虚拟内存的目的是为了让物理内存扩充为更大的逻辑内存，从而让程序获得更多的可用内存。

为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多块，每一块被称为一页。这些页被映射到物理内存，但不需映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序应用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。

从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。





#### 分页系统地址映射

内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。

一个虚拟地址分成两个部分，前四位存储页面号，后十二位存储偏移量。



#### 页面置换算法

在程序运行过程中，如果要访问的页面不在内存中，就发生<font color='cornflowerblue'>缺页中断</font>从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。

页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。



##### 最佳

> OPT, Optimal replacement algorithm

被置换的页面将是最长时间不再被访问，是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。

通常可以保证获得最低的缺页率。



##### 最近最少使用

> LRU, Least Recently Used

内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。

```
4，7，0，7，1，0，1，2，1，2，6
```

每次更新链表，代价高。



##### 最近未使用

> NRU, Not Recently Used

页面具备两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。

其中 R 位会定时被清零。可以将页面分成以下四类：

- R=0，M=0
- R=0，M=1
- R=1，M=0
- R=1，M=1

当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。

NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。



##### 先进先出

> FIFO, First In First Out

选择换出的页面是最先进入的页面，缺页率较高。



#####  第二次机会算法

改进 FIFO 算法可能会把经常使用的页面置换出去的问题。

页面具有一个状态位 R，被访问（读/写）时置为 1。

如果需要将其置换出去时，检查其状态位 R，为 1，则给予机会将 R 清 0，将页面放入链表尾端；状态位为 0，直接置换出去。然后继续从链表的头部开始搜索。



##### 时钟

第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面连接起来，再使用一个指针指向最老的页面。

当R=1 把指针向前一位







#### 分段

虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。



下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现



symbol table的内存大小使用满了，无法增长

分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。



#### 段页式

程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统的共享和保护，又拥有分页系统的虚拟内存功能。



#### 分页与分段的比较

- 对程序员的透明性：分页透明，但是分段需要程序员显式划分每个段。
- 地址空间的维度：分页是一维地址空间，分段是二维的。
- 大小是否可以改变：页的大小不可变，段的大小可以动态改变。
- 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。





**局部性原理**

1. **时间局部性** ：指令可能再次执行，数据可能再次被访问。程序中存在着大量的循环操作。
2. **空间局部性** ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问。因为数据结构的局限，聚簇存放







## 设备管理



### 磁盘结构

![Snipaste_2022-06-14_17-37-05](assets/Snipaste_2022-06-14_17-37-05.png)

- 盘面（Platter）：一个磁盘有多个盘面；
- 磁道（Track）：盘面上的圆形区域，一个盘面可以有多个磁道；
- 扇区（Track Sector）：磁道上的一个弧段，一个磁道可以有多个扇区，它是最小的物理储存单位，目前主要有 512 bytes 与 4 K 两种大小；
- 磁头（Head）：与盘面非常接近，能够将盘面上的磁场转换为电信号（读），或者将电信号转换为盘面的磁场（写）；
- 制动手臂（Actuator arm）：用于在磁道之间移动磁头；
- 主轴（Spindle）：使整个盘面转动。



### 磁盘调度算法

读写一个磁盘块的时间的影响因素有：

- 旋转时间（主轴转动盘面，使得磁头移动到适当的扇区上）
- 寻道时间（制动手臂移动，使得磁头移动到适当的磁道上）
- 实际的数据传输时间

其中，寻道时间最长，因此磁盘调度的主要目标是使磁盘的平均寻道时间最短。



#### 1. 先来先服务

> FCFS, First Come First Served

按照磁盘请求的顺序进行调度。

优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。



#### 2. 最短寻道时间优先

> SSTF, Shortest Seek Time First

优先调度与当前磁头所在磁道距离最近的磁道。

虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁道请求会一直等待下去，也就是出现饥饿现象。具体来说，两端的磁道请求更容易出现饥饿现象。



#### 3. 电梯算法

> SCAN

电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。

电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。

因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。



## 链接

### 编译系统

以下是一个 hello.c 程序：

```c
#include <stdio.h>

int main()
{
    printf("hello, world\n");
    return 0;
}
```

在 Unix 系统上，由编译器把源文件转换为目标文件。

```
gcc -o hello hello.c
```

这个过程大致如下：

![image-20220509030554882](assets\image-20220509030554882.png)

- 预处理阶段：处理以 # 开头的预处理命令；
- 编译阶段：翻译成汇编文件；
- 汇编阶段：将汇编文件翻译成可重定位目标文件；
- 链接阶段：将可重定位目标文件和 printf.o 等单独预编译好的目标文件进行合并，得到最终的可执行目标文件。

### 静态链接

静态链接器以一组可重定位目标文件为输入，生成一个完全链接的可执行目标文件作为输出。链接器主要完成以下两个任务：

- 符号解析：每个符号对应于一个函数、一个全局变量或一个静态变量，符号解析的目的是将每个符号引用与一个符号定义关联起来。
- 重定位：链接器通过把每个符号定义与一个内存位置关联起来，然后修改所有对这些符号的引用，使得它们指向这个内存位置。

![image-20220509030606998](assets\image-20220509030606998.png)



### 目标文件

- 可执行目标文件：可以直接在内存中执行；
- 可重定位目标文件：可与其它可重定位目标文件在链接阶段合并，创建一个可执行目标文件；
- 共享目标文件：这是一种特殊的可重定位目标文件，可以在运行时被动态加载进内存并链接；

### 动态链接

静态库有以下两个问题：

- 当静态库更新时那么整个程序都要重新进行链接；
- 对于 printf 这种标准函数库，如果每个程序都要有代码，这会极大浪费资源。

共享库是为了解决静态库的这两个问题而设计的，在 Linux 系统中通常用 .so 后缀来表示，Windows 系统上它们被称为 DLL。它具有以下特点：

- 在给定的文件系统中一个库只有一个文件，所有引用该库的可执行目标文件都共享这个文件，它不会被复制到引用它的可执行文件中；
- 在内存中，一个共享库的 .text 节（已编译程序的机器代码）的一个副本可以被不同的正在运行的进程共享。

![image-20220509030637847](assets\image-20220509030637847.png)





# I/O

操作系统为了系统的稳定和安全，将地址空间分为<span style='font-size: 24px'>用户空间（User space）</span> 和<span style='font-size: 24px'> 内核空间（Kernel space )</span>

一般来说，应用程序都运行在用户空间，需要进行 IO 操作时发起<span style='font-size: 24px'>系统调用</span>请求操作系统帮忙完成

1. 内核等待 I/O 设备准备好数据
2. 内核将数据从内核空间拷贝到用户空间



## 阻塞与同步

阻塞（Blocking）是指当一个线程执行某个操作时，如果该操作无法立即完成，线程将会被暂停，直到操作完成或满足某个条件才会继续执行。在阻塞状态下，线程无法执行其他任务，只能等待。

同步（Synchronization）是指多个线程之间的协调和互斥操作，确保它们按照一定的顺序执行。在同步操作中，线程之间会相互等待或者协作，以保证数据的一致性和正确性。

阻塞和同步的区别在于：

- 阻塞是指线程在执行过程中被暂停，等待某个操作完成或条件满足，而同步是指多个线程之间的协调和互斥操作。
- 阻塞是一种线程状态，而同步是一种线程间的行为。
- 阻塞是为了等待某个操作的完成，而同步是为了保证多个线程之间的数据一致性和正确性。

在编程中，阻塞和同步的使用取决于具体的需求和场景。阻塞通常用于等待用户输入、网络通信、文件读写等需要等待的操作。同步通常用于多线程之间的数据共享和协作，以避免数据竞争和不一致的情况发生。



<span style='font-size: 24px'>同步和异步</span>

同步是指程序按照顺序依次执行，每个操作都要等待前一个操作完成后才能执行。在同步执行中，程序会阻塞等待每个操作的完成，直到前一个操作完成后才能继续执行下一个操作。同步操作通常是阻塞的，即程序会一直等待操作完成才能继续执行。

异步是指程序在执行过程中不需要等待某个操作的完成，而是继续执行后续的操作。在异步执行中，程序不会阻塞等待操作的完成，而是通过回调函数、事件驱动或者轮询等方式来处理操作的结果。异步操作通常是非阻塞的，即程序可以继续执行其他操作而不需要等待当前操作的完成。

同步和异步的区别在于：

- 执行方式：同步按照顺序依次执行，每个操作都要等待前一个操作完成后才能执行；异步不需要等待操作的完成，可以继续执行后续的操作。
- 阻塞状态：同步操作通常是阻塞的，程序会等待操作完成才能继续执行；异步操作通常是非阻塞的，程序可以继续执行其他操作而不需要等待当前操作的完成。
- 处理方式：同步操作直接处理操作的结果；异步操作通过回调函数、事件驱动或者轮询等方式来处理操作的结果。

在编程中，同步和异步的选择取决于具体的需求和场景。同步适用于需要按照顺序执行的操作，或者需要等待操作完成后才能继续执行的情况。异步适用于需要并发执行多个操作，或者不需要等待操作完成的情况，可以提高程序的响应性和效率。







##  IO 模型

### BIO

应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。

![Snipaste_2022-05-10_16-08-57](assets/Snipaste_2022-05-10_16-08-57.png)



### NIO

应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）

CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低。

![Snipaste_2022-05-10_16-11-24](assets/Snipaste_2022-05-10_16-11-24.png)

数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的。



### I/O 多路复用

使用一个线程轮询多个 IO 调用中的数据是否就绪，相较于 NIO 减少了无效的系统调用，减少了对 CPU 资源的消耗。

![Snipaste_2022-05-10_16-16-30](assets/Snipaste_2022-05-10_16-16-30.png)

### 信号驱动 I/O

应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行。内核在数据就绪时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。

相比于 NIO 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。

![Snipaste_2022-05-10_16-19-59](assets/Snipaste_2022-05-10_16-19-59.png)



### AIO (Asynchronous I/O)

应用进程发起 IO 会立即返回，可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。

![yibu](assets/yibu.png)

> 异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。





## I/O 多路复用



### select

select 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I/O 操作。



### poll

poll 的功能与 select 类似，也是等待一组描述符中的一个成为就绪状态。



### 比较

#### 1. 功能

- select 会修改描述符，而 poll 不会；
- select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 没有描述符数量的限制；
- poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。
- 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。

#### 2. 速度

select 和 poll 速度都比较慢，每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。

#### 3. 可移植性

几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。



### epoll

epoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。

从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。

> epoll 仅适用于 Linux OS。

epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。

epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。



#### 工作模式

epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。

##### 1. LT 模式

当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。

##### 2. ET 模式

和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。

很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。



### 应用场景

很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。

#### 1. select 应用场景

select 的 timeout 参数精度为微秒，而 poll 和 epoll 为毫秒，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。

select 可移植性更好，几乎被所有主流平台所支持。

#### 2. poll 应用场景

poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。

#### 3. epoll 应用场景

只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。

需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。

需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。











## NIO

socket 的接收请求是阻塞的，需要处理完一个请求才能处理下一个请求，所以在面对高并发的服务请求时，性能就会很差。当然可以使用多线程，但是缺点也很明显就是消耗系统资源。

#### Reactor 模式

它是将客户端请求提交到一个或者多个服务处理程序的设计模式。

工作原理是由**一个线程来接收所有的请求**，然后**派发这些请求到相关的工作线程中**。



#### 未出现 Reactor 之前



![image-20221122154317196](assets/image-20221122154317196.png)



由于以上的问题，提出了Reactor模式。

提出了三种形式，**单Reactor单线程，单Reactor多线程和多Reactor多线程**。



## Reactor 三角色

`Reactor`：负责接收和分发事件，连接事件转发至 Acceptor，如果是普通事件则转发到 Handler

`Handler`：事件处理器

`Acceptor`：处理连接请求





## 单 Reactor 单线程

![image-20221122154521439](assets/image-20221122154521439.png)

只有一个 `select` 循环接收请求，客户端（client）注册进来由`Reactor`接收注册事件，然后再由reactor分发（dispatch）出去，由下面的处理器（Handler）去处理。



<h4>通俗解释</h4>

一个餐厅里只有一个既是前台也是服务员的人，负责接待客人，也负责把客人点的菜下达给厨师。



很明显，会忙不过来的。

不仅充当前台又充当服务员的 reactor 忙不过来，而且只有一个厨师（handler）， 发生问题，全部客人的菜都上不了。





## 单 Reactor 多线程

![image-20221122155034413](assets/image-20221122155034413.png)

在**多线程Reactor**中，注册接收事件都是由`Reactor`来做，其它的计算，编解码由一个线程池来做。从图中可以看出工作线程是多线程的，监听注册事件的`Reactor`还是单线程。



<h4>通俗解释</h4>

一个餐厅里只有一个既是前台也是服务员的人，负责接待客人，也负责把客人点的菜下达给厨师。有多名厨师。



很明显，厨师变多了。

那么，只剩下前台招呼不周了。



## 多 Reactor 多线程

![image-20221122155439359](assets/image-20221122155439359.png)



1、`mainReactor `负责监听客户端请求，专门处理新连接的建立，将建立好的连接注册到 subReactor。

2、`subReactor` 将分配的连接加入到队列进行监听，当有新的事件发生时，会调用连接相对应的 Handler 进行处理。



<h4>通俗解释</h4>

饭店有钱了，可以招聘服务员了。

前台和服务员分离，前台只负责招呼客人坐下，服务员等客人点菜，然后将点的菜告诉厨师，厨师负责做。









# 单核 CPU 如何支持 Java 多线程？

- 时间片
- 超线程
- 上下文切换
- 切换查看
- 线程调度
- 引起线程上下文切换的因素

------

由于现在大多计算机都是多核CPU，多线程往往会比单线程更快，更能够提高并发，但提高并发并不意味着启动更多的线程来执行。更多的线程意味着线程创建销毁开销加大、上下文非常频繁，你的程序反而不能支持更高的TPS。

## ***\*时间片\****

多任务系统往往需要同时执行多道作业。作业数往往大于机器的CPU数，然而一颗CPU同时只能执行一项任务，如何让用户感觉这些任务正在同时进行呢? 操作系统的设计者 巧妙地利用了时间片轮转的方式

时间片是CPU分配给各个任务（线程）的时间！

> “
>
> 思考：单核CPU为何也支持多线程呢？
>
> ”

线程上下文是指某一时间点 CPU 寄存器和程序计数器的内容，CPU通过时间片分配算法来循环执行任务（线程），因为时间片非常短，所以CPU通过不停地切换线程执行。

换言之，单CPU这么频繁，多核CPU一定程度上可以减少上下文切换。

## ***\*超线程\****

现代CPU除了处理器核心之外还包括寄存器、L1L2缓存这些存储设备、浮点运算单元、整数运算单元等一些辅助运算设备以及内部总线等。一个多核的CPU也就是一个CPU上有多个处理器核心，就意味着程序的不同线程需要经常在CPU之间的外部总线上通信，同时还要处理不同CPU之间不同缓存导致数据不一致的问题。

超线程这个概念是Intel提出的，简单来说是在一个CPU上真正的并发两个线程，由于CPU都是分时的（如果两个线程A和B，A正在使用处理器核心，B正在使用缓存或者其他设备，那AB两个线程就可以并发执行，但是如果AB都在访问同一个设备，那就只能等前一个线程执行完后一个线程才能执行）。实现这种并发的原理是 在CPU里加了一个协调辅助核心，根据Intel提供的数据，这样一个设备会使得设备面积增大5%，但是性能提高15%~30%。

## ***\*上下文切换\****

- 线程切换，同一进程中的两个线程之间的切换
- 进程切换，两个进程之间的切换
- 模式切换，在给定线程中，用户模式和内核模式的切换
- 地址空间切换，将虚拟内存切换到物理内存

CPU切换前把当前任务的状态保存下来，以便下次切换回这个任务时可以再次加载这个任务的状态，然后加载下一任务的状态并执行。任务的状态保存及再加载, 这段过程就叫做上下文切换。

每个线程都有一个程序计数器（记录要执行的下一条指令），一组寄存器（保存当前线程的工作变量），堆栈（记录执行历史，其中每一帧保存了一个已经调用但未返回的过程）。

寄存器 是 CPU 内部的数量较少但是速度很快的内存（与之对应的是 CPU 外部相对较慢的 RAM 主内存）。寄存器通过对常用值（通常是运算的中间值）的快速访问来提高计算机程序运行的速度。

程序计数器是一个专用的寄存器，用于表明指令序列中 CPU 正在执行的位置，存的值为正在执行的指令的位置或者下一个将要被执行的指令的位置。

- 挂起当前任务（线程/进程），将这个任务在 CPU 中的状态（上下文）存储于内存中的某处
- 恢复一个任务（线程/进程），在内存中检索下一个任务的上下文并将其在 CPU 的寄存器中恢复
- 跳转到程序计数器所指向的位置（即跳转到任务被中断时的代码行），以恢复该进程在程序中]

![图片](assets/640-1669810734365-4.png)

图片

##### 线程上下文切换会有什么问题呢？

上下文切换会导致额外的开销，常常表现为高并发执行时速度会慢串行，因此减少上下文切换次数便可以提高多线程程序的运行效率。

- 直接消耗：指的是CPU寄存器需要保存和加载, 系统调度器的代码需要执行, TLB实例需要重新加载, CPU 的pipeline需要刷掉
- 间接消耗：指的是多核的cache之间得共享数据, 间接消耗对于程序的影响要看线程工作区操作数据的大小

## ***\*切换查看\****

Linux系统下可以使用vmstat命令来查看上下文切换的次数， 其中cs列就是指上下文切换的数目（一般情况下, 空闲系统的上下文切换每秒大概在1500以下）

![图片](assets/640-1669810734357-1.png)

图片

## ***\*线程调度\****

##### 抢占式调度

指的是每条线程执行的时间、线程的切换都由系统控制，系统控制指的是在系统某种运行机制下，可能每条线程都分同样的执行时间片，也可能是某些线程执行的时间片较长，甚至某些线程得不到执行的时间片。在这种机制下，一个线程的堵塞不会导致整个进程堵塞。

java使用的线程调使用抢占式调度，Java中线程会按优先级分配CPU时间片运行，且优先级越高越优先执行，但优先级高并不代表能独自占用执行时间片，可能是优先级高得到越多的执行时间片，反之，优先级低的分到的执行时间少但不会分配不到执行时间。

![图片](assets/640-1669810734357-2.png)

图片

##### 协同式调度

指某一线程执行完后主动通知系统切换到另一线程上执行，这种模式就像接力赛一样，一个人跑完自己的路程就把接力棒交接给下一个人，下个人继续往下跑。线程的执行时间由线程本身控制，线程切换可以预知，不存在多线程同步问题，但它有一个致命弱点：如果一个线程编写有问题，运行到一半就一直堵塞，那么可能导致整个系统崩溃。

![图片](assets/640-1669810734357-3.png)

图片

##### 线程让出cpu的情况

- 当前运行线程主动放弃CPU，JVM暂时放弃CPU操作（基于时间片轮转调度的JVM操作系统不会让线程永久放弃CPU，或者说放弃本次时间片的执行权），例如调用`yield()`方法。
- 当前运行线程因为某些原因进入阻塞状态，例如阻塞在I/O上
- 当前运行线程结束，即运行完`run()`方法里面的任务

## ***\*[引起线程上下文切换的因素](https://mp.weixin.qq.com/s?__biz=MzUzMTA2NTU2Ng==&mid=2247487551&idx=1&sn=18f64ba49f3f0f9d8be9d1fdef8857d9&scene=21#wechat_redirect)\****

- 当前执行任务（线程）的时间片用完之后，系统CPU正常调度下一个任务
- 中断处理，在中断处理中，其他程序”打断”了当前正在运行的程序。当CPU接收到中断请求时，会在正在运行的程序和发起中断请求的程序之间进行一次上下文切换。中断分为硬件中断和软件中断，软件中断包括因为IO阻塞、未抢到资源或者用户代码等原因，线程被挂起。
- 用户态切换，对于一些操作系统，当进行用户态切换时也会进行一次上下文切换，虽然这不是必须的。
- 多个任务抢占锁资源，在多任务处理中，CPU会在不同程序之间来回切换，每个程序都有相应的处理时间片，CPU在两个时间片的间隔中进行上下文切换

##### 因此优化手段有：

- 无锁并发编程，多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据
- CAS算法，Java的Atomic包使用CAS算法来更新数据，而不需要加锁
- 使用最少线程
- 协程，单线程里实现多任务的调度，并在单线程里维持多个任务间的切换

合理设置线程数目既可以最大化利用CPU，又可以减少线程切换的开销。

- 高并发，低耗时的情况，建议少线程。
- 低并发，高耗时的情况：建议多线程。
- 高并发高耗时，要分析任务类型、增加排队、加大线程数
